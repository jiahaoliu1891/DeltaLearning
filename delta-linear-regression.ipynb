{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c3e224",
   "metadata": {},
   "source": [
    "# Delta Linear Regression\n",
    "\n",
    "* See this [Notion Page](https://www.notion.so/delta-linear-regression-1ae807742b3549f4af1d16e1f47c4203) for detail\n",
    "* Related to: [Sketch-RNN](https://github.com/jiahaoliu1891/Sketch-Composer)\n",
    "\n",
    "## 一个关于 Linear Regression 的简单分析\n",
    "\n",
    "Linear Regression on data $X, Y$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w x + b\n",
    "$$\n",
    "\n",
    "损失 $L$ 是 $X, \\hat{y}, w$ 三者的函数:\n",
    "\n",
    "$$\n",
    "L = \\frac12(y-\\hat{y})^2\n",
    "$$\n",
    "\n",
    " 梯度  $\\frac{\\partial L}{\\partial w}$  正比与 $x$ 的取值 \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = (y-\\hat{y})x\n",
    "$$\n",
    "\n",
    "当我们用 SGD 训练的时候,  $w$  受到学习率 $\\alpha$  和梯度的共同作用。 \n",
    "\n",
    "$$\n",
    "w := w - \\alpha  \\frac{\\partial L}{\\partial w} =w - \\alpha(y-\\hat{y})x \n",
    "$$\n",
    "\n",
    "假设世界的真实规律是:\n",
    "\n",
    "$$\n",
    "y = 2x + 1\n",
    "$$\n",
    "\n",
    "我们根据这个规律采集数据点：$(1, 3), (2, 5), (3, 7), \\dots,  (100, 201)$\n",
    "\n",
    "假设此时 $w = 1.9, b = 1.0$。考虑两个数据点$(1, 3)$ 和 $(100, 201)$\n",
    "\n",
    "对于第一个数据点 $(1, 3)$，预测误差 $(y-\\hat{y}) = -0.1$, 而 $x = 1$。因此我们调整参数 $w$ 的方法为：\n",
    "\n",
    "$$\n",
    "w := w - \\alpha\\frac{\\partial L}{\\partial w} = w + \\alpha \\times 0.1\n",
    "$$\n",
    "\n",
    "考虑到真实世界的 $w_{gt} = 2$, 此时**合理**的学习率 $\\alpha=1.0$ .\n",
    "\n",
    "但如果第二个数据点 $(100, 201)$，预测误差 $(\\hat{y}-y) = -10$, 而 $x = 100$。因此我们调整参数 $w$ 的方法为：\n",
    "\n",
    "$$\n",
    "w := w - \\alpha\\frac{\\partial L}{\\partial w} = w + \\alpha \\times 1000\n",
    "$$\n",
    "\n",
    "考虑到真实世界的 $w_{gt} = 2$, 此时合理的学习率 $\\alpha= 10^{-4}$.\n",
    "\n",
    "从上边的例子我们可以发现，$x$ 变大 $10^2$倍，梯度 $\\frac{\\partial L}{\\partial w}$变大 $10^4$ 倍，合理的学习率  $\\alpha$ 的值就得小 $10^4$ 。（这也就是为什么做ML需要经常调学习率的参数，以及我们 preprocess 的时候为什么要 normalize data）\n",
    "\n",
    "我们可以思考，到底什么是 $\\frac{\\partial L}{\\partial w}$? 为什么他会随 $x$ 的变化而变化如此剧烈？这需要我们思考参数空间，即不同的 $w$ 对预测结果的影响。在数据 $x$ 处给 $w$ 一个变化量$dw$，预测结果发生的变化为：\n",
    "\n",
    "$$\n",
    "dy=(w+dw)x - wx=x \\times dw \n",
    "$$\n",
    "\n",
    "我们可以发现，对于同样的变化 $dw$， 在不同数据点 $x$ 处，$dy$ 是不一样的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e342a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "def trainLR(model, X, y):\n",
    "    lr = 0.01\n",
    "    epochs = 10\n",
    "\n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    REGULARIZE = False\n",
    "    l1_lambda = 0.1\n",
    "\n",
    "    batch_size = 10\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Converting inputs and labels to Variable\n",
    "        s, e = 0, batch_size\n",
    "        while s < e and e <= N:\n",
    "            inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "            labels = Variable(torch.from_numpy(y[s:e]))\n",
    "\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            if REGULARIZE:\n",
    "                for W in model.parameters():\n",
    "                    loss +=  l1_lambda * W.norm(1).sum()\n",
    "\n",
    "            # get gradients w.r.t to parameters\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            s += batch_size\n",
    "            e = min(e + batch_size, N)\n",
    "\n",
    "        weight = model.linear.weight.detach().numpy()[0][0]\n",
    "        bias = model.linear.bias.detach().numpy()[0]\n",
    "        grad = model.linear.weight.grad.detach().numpy()[0][0]\n",
    "        print(f'epoch {epoch}, loss {loss.item():.2f}, weight:{weight:.2f}, bias:{bias:.2f}, grad:{grad:.2f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e53be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 64.36, weight:1.63, bias:-0.60, grad:-85.07\n",
      "epoch 1, loss 11.66, weight:1.99, bias:-0.53, grad:-35.28\n",
      "epoch 2, loss 2.54, weight:2.13, bias:-0.50, grad:-14.59\n",
      "epoch 3, loss 0.96, weight:2.19, bias:-0.48, grad:-5.99\n",
      "epoch 4, loss 0.69, weight:2.22, bias:-0.47, grad:-2.41\n",
      "epoch 5, loss 0.63, weight:2.22, bias:-0.46, grad:-0.93\n",
      "epoch 6, loss 0.62, weight:2.23, bias:-0.45, grad:-0.31\n",
      "epoch 7, loss 0.61, weight:2.23, bias:-0.44, grad:-0.05\n",
      "epoch 8, loss 0.60, weight:2.23, bias:-0.43, grad:0.05\n",
      "epoch 9, loss 0.59, weight:2.23, bias:-0.42, grad:0.10\n",
      "=== Look at the parameter, the model has already capture the law: y = 2 * X + b ===\n"
     ]
    }
   ],
   "source": [
    "x_values = [i for i in range(10)]\n",
    "X = np.array(x_values, dtype=np.float32)\n",
    "# np.random.shuffle(X)\n",
    "X = X.reshape(-1, 1)\n",
    "y = X * 2 + 1\n",
    "n_feat = X.shape[-1]        # takes variable 'x' \n",
    "n_out = y.shape[-1]        # takes variable 'y'\n",
    "\n",
    "\n",
    "# create model\n",
    "model = linearRegression(n_feat, n_out)\n",
    "\n",
    "trainLR(model, X, y)\n",
    "\n",
    "print('=== Look at the parameter, the model has already capture the law: y = 2 * X + b ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de98e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Now we shift the data distribution, while not change the data generation function y = 2 * X + b ===\n",
      "epoch 0, loss 51306.35, weight:-4548.36, bias:-4.95, grad:455058.53\n",
      "epoch 1, loss 20892759359488.00, weight:91824400.00, bias:91411.87, grad:-9182895104.00\n",
      "epoch 2, loss 8507862646056288256000.00, weight:-1852978626560.00, bias:-1844662528.00, grad:185307051851776.00\n",
      "epoch 3, loss 3464536289305600348642728738816.00, weight:37392350351196160.00, bias:37224531886080.00, grad:-3739420354368503808.00\n",
      "epoch 4, loss inf, weight:-754562284598486630400.00, bias:-751175761297145856.00, grad:75459968980814170947584.00\n",
      "epoch 5, loss inf, weight:15226758826865125551505408.00, bias:15158418287786847109120.00, grad:-1522751274529541806087995392.00\n",
      "epoch 6, loss inf, weight:-307269728814764986676455407616.00, bias:-305890753972303131140161536.00, grad:30728495825777758196748376866816.00\n",
      "epoch 7, loss inf, weight:6200577975552102010971159724556288.00, bias:6172749243047174851522478473216.00, grad:-620088483612904007103370171946893312.00\n",
      "epoch 8, loss inf, weight:-inf, bias:-124563566878079705567042913175601152.00, grad:inf\n",
      "epoch 9, loss inf, weight:nan, bias:inf, grad:-inf\n"
     ]
    }
   ],
   "source": [
    "x_values = [i for i in range(10)]\n",
    "# NOTE: We shift x_values by 1000, nothing else changes\n",
    "print('=== Now we shift the data distribution, while not change the data generation function y = 2 * X + b ===')\n",
    "\n",
    "X = np.array(x_values, dtype=np.float32) + 1000\n",
    "# np.random.shuffle(X)\n",
    "X = X.reshape(-1, 1)\n",
    "y = X * 2 + 1\n",
    "trainLR(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baead163",
   "metadata": {},
   "source": [
    "## A simple solution\n",
    "\n",
    "一个简单的解决方案就是预测相对值，而不是绝对值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "034d38d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- TRAIN ----\n",
      "epoch 0, loss 0.28, weight:10.00, bias:0.54, grad:5.53\n",
      "epoch 1, loss 0.28, weight:10.00, bias:0.54, grad:5.50\n",
      "epoch 2, loss 0.28, weight:10.00, bias:0.54, grad:5.52\n",
      "epoch 3, loss 0.28, weight:10.00, bias:0.54, grad:5.49\n",
      "epoch 4, loss 0.28, weight:10.00, bias:0.54, grad:5.53\n",
      "----- TRAIN ----\n",
      "epoch 0, loss 0.29, weight:10.00, bias:0.54, grad:-19.56\n",
      "epoch 1, loss 0.29, weight:10.00, bias:0.54, grad:-19.50\n",
      "epoch 2, loss 0.29, weight:10.00, bias:0.54, grad:-19.51\n",
      "epoch 3, loss 0.29, weight:10.00, bias:0.53, grad:-19.49\n",
      "epoch 4, loss 0.29, weight:10.00, bias:0.53, grad:-19.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class DelatlinearRegression(nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(DelatlinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train(model, X, y):\n",
    "    print('----- TRAIN ----')\n",
    "    lr = 0.00001\n",
    "    epochs = 5\n",
    "\n",
    "    ##### For GPU #######\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    REGULARIZE = False\n",
    "    l1_lambda = 0.1\n",
    "\n",
    "    batch_size = 10\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Converting inputs and labels to Variable\n",
    "        s, e = 0, batch_size - 1\n",
    "        while s < e and e <= N:\n",
    "            delta_X = Variable(torch.from_numpy(X[s:e] - X[s+1: e+1]))\n",
    "            delta_y = Variable(torch.from_numpy(y[s:e] - y[s+1: e+1]))\n",
    "            \n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(delta_X)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = loss_func(outputs, delta_y)\n",
    "\n",
    "            if REGULARIZE:\n",
    "                for W in model.parameters():\n",
    "                    loss +=  l1_lambda * W.norm(1).sum()\n",
    "\n",
    "            # get gradients w.r.t to parameters\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            s += batch_size\n",
    "            e = min(e + batch_size, N - 1)\n",
    "\n",
    "        weight = model.linear.weight.detach().numpy()[0][0]\n",
    "        bias = model.linear.bias.detach().numpy()[0]\n",
    "        grad = model.linear.weight.grad.detach().numpy()[0][0]\n",
    "        print(f'epoch {epoch}, loss {loss.item():.2f}, weight:{weight:.2f}, bias:{bias:.2f}, grad:{grad:.2f}')\n",
    "    \n",
    "\n",
    "    # with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         pred = model(Variable(torch.from_numpy(X).cuda())).cpu().data.numpy()\n",
    "    #     else:\n",
    "    #         pred = model(Variable(torch.from_numpy(X))).data.numpy()\n",
    "\n",
    "    # plt.plot(X, y, 'go', label='True data', alpha=0.5)\n",
    "    # plt.plot(X, pred, '--', label='Predictions', alpha=0.5)\n",
    "    # plt.legend(loc='best')\n",
    "    # plt.show()\n",
    "\n",
    "def main():\n",
    "    # create dummy data for training\n",
    "    x_values = [i for i in range(500)]\n",
    "    X = np.array(x_values, dtype=np.float32)\n",
    "    np.random.shuffle(X)\n",
    "    X = X.reshape(-1, 1)\n",
    "    y = X * 10 + 2\n",
    "    n_feat = X.shape[-1]        # takes variable 'x' \n",
    "    n_out = y.shape[-1]       # takes variable 'y'\n",
    "\n",
    "    # create model\n",
    "    model = DelatlinearRegression(n_feat, n_out)\n",
    "\n",
    "    train(model, X, y)\n",
    "    '''NOTE: Already Capture the laws !!!'''\n",
    "\n",
    "    \n",
    "    x_values = [i for i in range(500)]\n",
    "    # NOTE: We shift x_values by 1000, nothing else changes\n",
    "    X = np.array(x_values, dtype=np.float32) + 1000\n",
    "    np.random.shuffle(X)\n",
    "    X = X.reshape(-1, 1)\n",
    "    y = X * 10 + 2\n",
    "    train(model, X, y)\n",
    "    '''NOTE: NOT EXLPODE !!!'''\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ac6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3fd19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Her",
   "language": "python",
   "name": "her"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
