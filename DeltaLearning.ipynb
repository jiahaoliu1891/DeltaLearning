{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c3e224",
   "metadata": {},
   "source": [
    "# Delta Learning\n",
    "\n",
    "Delta Learning mainly concerns about **Domain Generalization** Problem.\n",
    "That is: \n",
    "1. if the training and testing data are from **different domain** (or say distribution), \n",
    "2. but they contains the **same pattern**\n",
    "\n",
    "how can we build model that can generalize accross different domains?\n",
    "\n",
    "For example, when a child learns to recognize cat in real life, he can easily recognize cat in different domains without any hints or instructions:\n",
    "* Emoji cat: üê±\n",
    "* Cartoon cat: <img src='https://ctl.s6img.com/society6/img/5uFHiOtud7B5teZ02cp0Mo5O_FY/w_700/prints/~artwork/s6-original-art-uploads/society6/uploads/misc/07c252f2a6f04364ab7d484376e803be/~~/calico-cat1907816-prints.jpg?wait=0&attempt=0' width=10%/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e342a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "def trainLR(model, X, y):\n",
    "    lr = 0.01\n",
    "    epochs = 10\n",
    "\n",
    "    loss_func = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    REGULARIZE = False\n",
    "    l1_lambda = 0.1\n",
    "\n",
    "    batch_size = 10\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Converting inputs and labels to Variable\n",
    "        s, e = 0, batch_size\n",
    "        while s < e and e <= N:\n",
    "            inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "            labels = Variable(torch.from_numpy(y[s:e]))\n",
    "\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get output from the model, given the inputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # get loss for the predicted output\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            if REGULARIZE:\n",
    "                for W in model.parameters():\n",
    "                    loss +=  l1_lambda * W.norm(1).sum()\n",
    "\n",
    "            # get gradients w.r.t to parameters\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            s += batch_size\n",
    "            e = min(e + batch_size, N)\n",
    "\n",
    "        weight = model.linear.weight.detach().numpy()[0][0]\n",
    "        bias = model.linear.bias.detach().numpy()[0]\n",
    "        grad = model.linear.weight.grad.detach().numpy()[0][0]\n",
    "        print(f'epoch {epoch}, loss {loss.item():.2f}, weight:{weight:.2f}, bias:{bias:.2f}, grad:{grad:.2f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e53be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 242.57, weight:0.83, bias:0.71, grad:-166.26\n",
      "epoch 1, loss 41.91, weight:1.52, bias:0.82, grad:-69.10\n",
      "epoch 2, loss 7.24, weight:1.81, bias:0.87, grad:-28.72\n",
      "epoch 3, loss 1.25, weight:1.93, bias:0.89, grad:-11.93\n",
      "epoch 4, loss 0.22, weight:1.98, bias:0.90, grad:-4.95\n",
      "epoch 5, loss 0.04, weight:2.00, bias:0.90, grad:-2.05\n",
      "epoch 6, loss 0.01, weight:2.01, bias:0.90, grad:-0.85\n",
      "epoch 7, loss 0.00, weight:2.01, bias:0.90, grad:-0.35\n",
      "epoch 8, loss 0.00, weight:2.01, bias:0.90, grad:-0.14\n",
      "epoch 9, loss 0.00, weight:2.01, bias:0.90, grad:-0.05\n",
      "=== Look at the parameter, the model has already capture the law: y = 2 * X + b ===\n"
     ]
    }
   ],
   "source": [
    "x_values = [i for i in range(10)]\n",
    "X = np.array(x_values, dtype=np.float32)\n",
    "# np.random.shuffle(X)\n",
    "X = X.reshape(-1, 1)\n",
    "y = X * 2 + 1\n",
    "n_feat = X.shape[-1]        # takes variable 'x' \n",
    "n_out = y.shape[-1]        # takes variable 'y'\n",
    "\n",
    "\n",
    "# create model\n",
    "model = linearRegression(n_feat, n_out)\n",
    "\n",
    "trainLR(model, X, y)\n",
    "\n",
    "print('=== Look at the parameter, the model has already capture the law: y = 2 * X + b ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5de98e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Now we shift the data distribution, while not change the data generation function y = 2 * X + b ===\n",
      "epoch 0, loss 221.40, weight:-296.92, bias:0.61, grad:29893.29\n",
      "epoch 1, loss 90158931968.00, weight:6032046.50, bias:6005.88, grad:-603234368.00\n",
      "epoch 2, loss 36714140208742793216.00, weight:-121724190720.00, bias:-121177896.00, grad:12173022593024.00\n",
      "epoch 3, loss 14950580188232114174024482816.00, weight:2456344409931776.00, bias:2445320126464.00, grad:-245646600045592576.00\n",
      "epoch 4, loss 6088115114101711869062580563582386176.00, weight:-49568020475676721152.00, bias:-49345562163544064.00, grad:4957047933552014917632.00\n",
      "epoch 5, loss inf, weight:1000262303490725912772608.00, bias:995773180684357599232.00, grad:-100031186758620896112410624.00\n",
      "epoch 6, loss inf, weight:-20184881893226967581579018240.00, bias:-20094294268065837295337472.00, grad:2018588322381599526115013033984.00\n",
      "epoch 7, loss inf, weight:407322622033045568305146863026176.00, bias:405494573869134977549715111936.00, grad:-40734280819281116553725443687055360.00\n",
      "epoch 8, loss inf, weight:-inf, bias:-8182713716064143993097679320121344.00, grad:inf\n",
      "epoch 9, loss inf, weight:nan, bias:inf, grad:-inf\n"
     ]
    }
   ],
   "source": [
    "x_values = [i for i in range(10)]\n",
    "# NOTE: We shift x_values by 1000, nothing else changes\n",
    "print('=== Now we shift the data distribution, while not change the data generation function y = 2 * X + b ===')\n",
    "\n",
    "X = np.array(x_values, dtype=np.float32) + 1000\n",
    "# np.random.shuffle(X)\n",
    "X = X.reshape(-1, 1)\n",
    "y = X * 2 + 1\n",
    "trainLR(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f950caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Her",
   "language": "python",
   "name": "her"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
